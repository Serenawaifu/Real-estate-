# Robots.txt for Real Estate Agent Website
# Last Updated: 2025-01-01
# ================================================

# Default - Allow all search engines
User-agent: *
Allow: /
Disallow: /api/
Disallow: /admin/
Disallow: /private/
Disallow: /temp/
Disallow: /test/
Disallow: /.env
Disallow: /config/
Disallow: /*.json$
Disallow: /*.log$
Disallow: /node_modules/
Disallow: /src/
Disallow: /scripts/

# Allow search engines to access assets
Allow: /assets/images/
Allow: /assets/animations/
Allow: /*.css$
Allow: /*.js$
Allow: /*.jpg$
Allow: /*.jpeg$
Allow: /*.png$
Allow: /*.gif$
Allow: /*.webp$
Allow: /*.svg$
Allow: /*.woff$
Allow: /*.woff2$

# Specific crawler rules
# ================================================

# Google
User-agent: Googlebot
Allow: /
Disallow: /api/
Disallow: /admin/
Crawl-delay: 0

# Google Images
User-agent: Googlebot-Image
Allow: /assets/images/
Allow: /*.jpg$
Allow: /*.jpeg$
Allow: /*.png$
Allow: /*.gif$
Allow: /*.webp$
Disallow: /assets/images/placeholders/
Disallow: /assets/images/temp/

# Bing
User-agent: Bingbot
Allow: /
Disallow: /api/
Disallow: /admin/
Crawl-delay: 1

# Yandex
User-agent: Yandex
Allow: /
Disallow: /api/
Disallow: /admin/
Crawl-delay: 2

# Baidu - Chinese search engine
User-agent: Baiduspider
Allow: /
Disallow: /api/
Disallow: /admin/
Crawl-delay: 2

# DuckDuckGo
User-agent: DuckDuckBot
Allow: /
Disallow: /api/
Disallow: /admin/
Crawl-delay: 1

# Facebook
User-agent: facebookexternalhit
Allow: /
Disallow: /api/
Disallow: /admin/

# Twitter
User-agent: Twitterbot
Allow: /
Disallow: /api/
Disallow: /admin/

# LinkedIn
User-agent: LinkedInBot
Allow: /
Disallow: /api/
Disallow: /admin/

# WhatsApp
User-agent: WhatsApp
Allow: /
Disallow: /api/
Disallow: /admin/

# Block bad bots
# ================================================

# Aggressive crawlers
User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: DotBot
Disallow: /

User-agent: MJ12bot
Disallow: /

# Content scrapers
User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: PerplexityBot
Disallow: /

User-agent: Claude-Web
Disallow: /

# SEO analysis bots (allow with restrictions)
User-agent: Screaming Frog SEO Spider
Allow: /
Crawl-delay: 10

# Additional Protection Rules
# ================================================

# Block access to sensitive file types
User-agent: *
Disallow: /*.sql$
Disallow: /*.db$
Disallow: /*.log$
Disallow: /*.sh$
Disallow: /*.env$
Disallow: /*.bak$
Disallow: /*.old$
Disallow: /*.tmp$
Disallow: /*.cache$
Disallow: /*.lock$

# Block access to version control
User-agent: *
Disallow: /.git/
Disallow: /.svn/
Disallow: /.hg/
Disallow: /.github/

# Block access to documentation and development files
User-agent: *
Disallow: /docs/
Disallow: /documentation/
Disallow: /.vscode/
Disallow: /.idea/
Disallow: /tests/
Disallow: /test/
Disallow: /spec/
Disallow: /specs/

# Sitemap location
Sitemap: https://yourdomain.com/sitemap.xml
Sitemap: https://yourdomain.com/sitemap-images.xml

# Host preference (optional - specify preferred domain)
Host: https://yourdomain.com

# ================================================
# Notes:
# - Update 'yourdomain.com' with your actual domain
# - Crawl-delay is in seconds
# - Review and update bot rules based on your needs
# - Monitor server logs for unwanted bot activity
# ================================================
